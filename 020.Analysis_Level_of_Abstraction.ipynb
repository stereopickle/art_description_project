{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Level of Abstraction\n",
    "\n",
    "Using Wordnet, I will calculate the average level of abstraction of each description (estimated by the number of hypernyms of each word), and see if this changes across different time period or different culture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"pickles/cleaned_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning: macro culture\n",
    "\n",
    "Adding a new column to categorize the culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnd = [\n",
    "    df.culture.isin(['Korean', 'Japanese', 'Chinese', 'Tibetan', 'Thai']),\n",
    "    (df.culture.str.contains(\"Italian\") | df.culture.isin(['Minoan', 'German', 'British', 'Roman', 'French', \n",
    "                                                           'Spanish', 'Flemish?', 'European', \n",
    "                                                          'Dutch', 'Greek', 'Irish', 'English', 'Russian'])),\n",
    "    df.culture.isin(['American', 'Canadian']), \n",
    "    df.culture.isin(['Indian', 'Mughal'])]\n",
    "vals = ['East_Asian', 'European', 'North_American', 'South_Asian']\n",
    "df['macro_culture'] = np.select(cnd, vals, default='others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "South_Asian       0.318565\n",
       "East_Asian        0.308017\n",
       "North_American    0.225738\n",
       "European          0.105485\n",
       "others            0.042194\n",
       "Name: macro_culture, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.macro_culture.value_counts('macro_culture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning: periods\n",
    "Cleaning up time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with '0' values in date end\n",
    "cnd = [\n",
    "    ((df.dateend == 0) & (df.century.isin([\"19th-20th century\", \"20th century\"]))),\n",
    "    ((df.dateend == 0) & (df.century.isin([\"19th century\", \"18th-19th century\"]))),\n",
    "    ((df.dateend == 0) & (df.century.isin([\"18th century\"]))),\n",
    "    ((df.dateend == 0) & (df.century.isin([\"17th-16th century BCE\", \"17th century\", \"16th-17th century\"]))),\n",
    "    ((df.dateend == 0) & (df.century.isin([\"16th century\"]))),\n",
    "    ((df.dateend == 0) & (df.century.isin([\"15th century\"]))), \n",
    "    ((df.dateend == 0) & (df.century.isin([\"2nd century CE\"])))]\n",
    "vals = [1900, 1800, 1700, 1600, 1500, 1500, 200]\n",
    "df['date'] = np.select(cnd, vals, default=df.dateend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-binning for each time period\n",
    "bins = [-2000, 1500, 1700, 1800, 1860, 1900, 2000, 2020]\n",
    "df['date_bins'] = pd.cut(df['date'], bins, labels = [\"Pre_15th\", \"16th-17th\", \"18th\", \"19th_1st\", \"19th_2nd\", \"20th\", \"21st\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19th_2nd     0.221519\n",
       "20th         0.217300\n",
       "16th-17th    0.175105\n",
       "18th         0.154008\n",
       "19th_1st     0.118143\n",
       "Pre_15th     0.073840\n",
       "21st         0.040084\n",
       "Name: date_bins, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date_bins.value_counts('date_bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypenym Level\n",
    "Calculating the level of hypernym for each unique word using Wordnet corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypernym count data\n",
    "First, making a dictionary with the number of hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4385"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = set([x for sub in df.description for x in sub]) # all unique value of nested list\n",
    "len(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypernym (x):\n",
    "    return x.hypernyms()\n",
    "\n",
    "def find_hyp_level(word):\n",
    "    skipped = {}\n",
    "    n1 = wn.synsets(word, pos=wn.NOUN) # noun only\n",
    "    if len(n1) == 0 : # if the word doesn't exist in Wordnet, return -1        \n",
    "        skipped[word] = wn.morphy(word) # skipped words and its morphed form\n",
    "        return np.nan\n",
    "    \n",
    "    elif len(n1) == 1 : # if there's only one synset, take the length of all hypernym\n",
    "        return len(list(n1[0].closure(hypernym)))\n",
    "    \n",
    "    else : # if there's more than one synset, take an average of all options \n",
    "        min_ = min([len(list(x.closure(hypernym))) for x in n1])\n",
    "        if min_ == 0: # if 0, it's likely they are proper nouns, therefore most concrete\n",
    "            return -1\n",
    "        else: return min_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 13,\n",
       " 'fruit': 7,\n",
       " 'food': 4,\n",
       " 'matter': 2,\n",
       " 'chocolate': 5,\n",
       " 'cake': 6,\n",
       " 'coin': 8,\n",
       " 'cloud': 3,\n",
       " 'happiness': 5,\n",
       " 'sky': 6,\n",
       " 'joy': 6,\n",
       " 'mountain': 5,\n",
       " 'landscape': 7,\n",
       " 'alps': -1,\n",
       " 'abstraction': 1,\n",
       " 'light': 5}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testwords = ['apple', 'fruit', 'food', 'matter', 'chocolate', 'cake', 'coin', \n",
    "             'cloud', 'happiness', 'sky', 'joy', 'mountain', 'landscape', 'alps', 'abstraction', 'light']\n",
    "{k : find_hyp_level(k) for k in testwords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict = {k : find_hyp_level(k) for k in allwords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_df = pd.DataFrame.from_dict(hyp_dict, 'index', columns = ['value']) # for easy data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_df = hyp_df.replace(-1, max(hyp_df.value)+1)\n",
    "#hyp_df.reset_index(inplace = True)\n",
    "#hyp_df.rename(columns = {'index': 'words'}, inplace = True)\n",
    "#hyp_df = hyp_df[hyp_df.value != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_score(list_):\n",
    "    return np.nansum([hyp_df.loc[word] for word in list_])/len(list_)\n",
    "    #return np.nansum([hyp_df.value[hyp_df.words == word] for word in list_])/len(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hyp_score'] = df.description.apply(lambda x: hyp_score(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations / future direction\n",
    "Currently I'm just averaging all occurrence in WordNet, this can be problematic for words that have multiple meanings especially if the abstraction level of each meaning is far apart. I also used only nouns, since different parts of speech have different level of hyper/hyponyms to gauge level of categorization accurately. In this process, some verbs that take same form as the noun cannot be filtered, possibly adding noise to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average abstraction per culture\n",
    "Hypothetically the level of hypernym is on average less for East Asian culture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_culture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>East_Asian</td>\n",
       "      <td>146.0</td>\n",
       "      <td>5.323014</td>\n",
       "      <td>1.194164</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.740439</td>\n",
       "      <td>5.319338</td>\n",
       "      <td>5.896259</td>\n",
       "      <td>10.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>European</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.445204</td>\n",
       "      <td>1.770812</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4.377660</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>North_American</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.711290</td>\n",
       "      <td>2.721985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>5.565034</td>\n",
       "      <td>7.145833</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>South_Asian</td>\n",
       "      <td>151.0</td>\n",
       "      <td>5.933817</td>\n",
       "      <td>0.936742</td>\n",
       "      <td>3.254237</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>5.784615</td>\n",
       "      <td>6.583333</td>\n",
       "      <td>8.171429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>others</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.957926</td>\n",
       "      <td>1.271037</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>4.370629</td>\n",
       "      <td>4.701389</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>7.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count      mean       std       min       25%       50%  \\\n",
       "macro_culture                                                             \n",
       "East_Asian      146.0  5.323014  1.194164  2.000000  4.740439  5.319338   \n",
       "European         42.0  5.445204  1.770812  1.666667  4.377660  5.450000   \n",
       "North_American  100.0  5.711290  2.721985  0.000000  3.875000  5.565034   \n",
       "South_Asian     151.0  5.933817  0.936742  3.254237  5.375000  5.784615   \n",
       "others           20.0  4.957926  1.271037  2.750000  4.370629  4.701389   \n",
       "\n",
       "                     75%        max  \n",
       "macro_culture                        \n",
       "East_Asian      5.896259  10.142857  \n",
       "European        6.400000   9.500000  \n",
       "North_American  7.145833  13.000000  \n",
       "South_Asian     6.583333   8.171429  \n",
       "others          5.583333   7.142857  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('macro_culture').hyp_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>C(macro_culture)</td>\n",
       "      <td>38.511221</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.550301</td>\n",
       "      <td>0.00725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Residual</td>\n",
       "      <td>1231.169797</td>\n",
       "      <td>454.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sum_sq     df         F   PR(>F)\n",
       "C(macro_culture)    38.511221    4.0  3.550301  0.00725\n",
       "Residual          1231.169797  454.0       NaN      NaN"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl = df.dropna(subset = ['hyp_score', 'macro_culture'], axis = 0)\n",
    "lm = ols('hyp_score ~ C(macro_culture)', data=df_cl).fit()\n",
    "\n",
    "sm.stats.anova_lm(lm, typ=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>\n",
       "<tr>\n",
       "      <th>group1</th>         <th>group2</th>     <th>meandiff</th>  <th>p-adj</th>  <th>lower</th>   <th>upper</th> <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>East_Asian</td>      <td>European</td>     <td>0.1222</td>    <td>0.9</td>  <td>-0.6675</td> <td>0.9119</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>East_Asian</td>   <td>North_American</td>  <td>0.3883</td>   <td>0.366</td> <td>-0.1972</td> <td>0.9737</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>East_Asian</td>     <td>South_Asian</td>   <td>0.6108</td>  <td>0.0129</td> <td>0.0873</td>  <td>1.1343</td>  <td>True</td> \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>East_Asian</td>       <td>others</td>      <td>-0.3651</td> <td>0.8774</td> <td>-1.4404</td> <td>0.7103</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>European</td>    <td>North_American</td>  <td>0.2661</td>    <td>0.9</td>  <td>-0.5632</td> <td>1.0954</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>European</td>      <td>South_Asian</td>   <td>0.4886</td>  <td>0.4361</td> <td>-0.2982</td> <td>1.2754</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>European</td>        <td>others</td>      <td>-0.4873</td> <td>0.7872</td> <td>-1.7126</td>  <td>0.738</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>North_American</td>   <td>South_Asian</td>   <td>0.2225</td>  <td>0.8104</td> <td>-0.359</td>   <td>0.804</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>North_American</td>     <td>others</td>      <td>-0.7534</td> <td>0.3365</td> <td>-1.8581</td> <td>0.3514</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>South_Asian</td>      <td>others</td>      <td>-0.9759</td> <td>0.0945</td> <td>-2.0491</td> <td>0.0973</td>  <td>False</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tukey_ = sts.multicomp.pairwise_tukeyhsd(df_cl.hyp_score, df_cl.macro_culture)\n",
    "tukey_._results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average abstraction per time period\n",
    "Hypothetically the level of hypernym should get much lower around the industrial revolution and on when the abstract art started to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Pre_15th</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.982248</td>\n",
       "      <td>1.622381</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.970000</td>\n",
       "      <td>5.289080</td>\n",
       "      <td>5.913150</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16th-17th</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5.440267</td>\n",
       "      <td>1.305792</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.853821</td>\n",
       "      <td>5.463186</td>\n",
       "      <td>5.970486</td>\n",
       "      <td>10.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18th</td>\n",
       "      <td>73.0</td>\n",
       "      <td>5.772908</td>\n",
       "      <td>1.394005</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.980392</td>\n",
       "      <td>5.626263</td>\n",
       "      <td>6.823529</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19th_1st</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.001700</td>\n",
       "      <td>1.184926</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>5.372262</td>\n",
       "      <td>6.061947</td>\n",
       "      <td>6.622712</td>\n",
       "      <td>8.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19th_2nd</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5.538377</td>\n",
       "      <td>1.964474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.481959</td>\n",
       "      <td>6.690382</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20th</td>\n",
       "      <td>97.0</td>\n",
       "      <td>5.742986</td>\n",
       "      <td>2.028446</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.387097</td>\n",
       "      <td>11.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21st</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.254755</td>\n",
       "      <td>0.888468</td>\n",
       "      <td>3.826087</td>\n",
       "      <td>4.990253</td>\n",
       "      <td>5.386256</td>\n",
       "      <td>5.521021</td>\n",
       "      <td>7.956522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count      mean       std       min       25%       50%       75%  \\\n",
       "date_bins                                                                      \n",
       "Pre_15th    34.0  4.982248  1.622381  1.666667  3.970000  5.289080  5.913150   \n",
       "16th-17th   78.0  5.440267  1.305792  2.000000  4.853821  5.463186  5.970486   \n",
       "18th        73.0  5.772908  1.394005  1.500000  4.980392  5.626263  6.823529   \n",
       "19th_1st    54.0  6.001700  1.184926  3.666667  5.372262  6.061947  6.622712   \n",
       "19th_2nd   104.0  5.538377  1.964474  0.000000  4.500000  5.481959  6.690382   \n",
       "20th        97.0  5.742986  2.028446  1.500000  4.500000  5.500000  6.387097   \n",
       "21st        19.0  5.254755  0.888468  3.826087  4.990253  5.386256  5.521021   \n",
       "\n",
       "                 max  \n",
       "date_bins             \n",
       "Pre_15th    7.666667  \n",
       "16th-17th  10.142857  \n",
       "18th        9.500000  \n",
       "19th_1st    8.357143  \n",
       "19th_2nd   13.000000  \n",
       "20th       11.666667  \n",
       "21st        7.956522  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('date_bins').hyp_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>C(date_bins)</td>\n",
       "      <td>30.497311</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.854014</td>\n",
       "      <td>0.087213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Residual</td>\n",
       "      <td>1239.183707</td>\n",
       "      <td>452.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sum_sq     df         F    PR(>F)\n",
       "C(date_bins)    30.497311    6.0  1.854014  0.087213\n",
       "Residual      1239.183707  452.0       NaN       NaN"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl = df.dropna(subset = ['hyp_score', 'date_bins'], axis = 0)\n",
    "lm = ols('hyp_score ~ C(date_bins)', data=df_cl).fit()\n",
    "\n",
    "sm.stats.anova_lm(lm, typ=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>\n",
       "<tr>\n",
       "   <th>group1</th>    <th>group2</th>  <th>meandiff</th>  <th>p-adj</th>  <th>lower</th>   <th>upper</th> <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16th-17th</td>   <td>18th</td>    <td>0.3326</td>  <td>0.8733</td> <td>-0.4659</td> <td>1.1312</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16th-17th</td> <td>19th_1st</td>  <td>0.5614</td>  <td>0.4725</td> <td>-0.3067</td> <td>1.4295</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16th-17th</td> <td>19th_2nd</td>  <td>0.0981</td>    <td>0.9</td>  <td>-0.6364</td> <td>0.8326</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16th-17th</td>   <td>20th</td>    <td>0.3027</td>  <td>0.8918</td> <td>-0.4431</td> <td>1.0485</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16th-17th</td>   <td>21st</td>    <td>-0.1855</td>   <td>0.9</td>  <td>-1.4401</td> <td>1.0691</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16th-17th</td> <td>Pre_15th</td>  <td>-0.458</td>  <td>0.8075</td> <td>-1.4658</td> <td>0.5497</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>18th</td>    <td>19th_1st</td>  <td>0.2288</td>    <td>0.9</td>  <td>-0.6514</td>  <td>1.109</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>18th</td>    <td>19th_2nd</td>  <td>-0.2345</td>   <td>0.9</td>  <td>-0.9833</td> <td>0.5142</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>18th</td>      <td>20th</td>    <td>-0.0299</td>   <td>0.9</td>  <td>-0.7897</td> <td>0.7299</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>18th</td>      <td>21st</td>    <td>-0.5182</td> <td>0.8842</td> <td>-1.7811</td> <td>0.7448</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>18th</td>    <td>Pre_15th</td>  <td>-0.7907</td> <td>0.2461</td> <td>-1.8088</td> <td>0.2275</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_1st</td>  <td>19th_2nd</td>  <td>-0.4633</td> <td>0.6188</td> <td>-1.2858</td> <td>0.3592</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_1st</td>    <td>20th</td>    <td>-0.2587</td>   <td>0.9</td>  <td>-1.0913</td> <td>0.5739</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_1st</td>    <td>21st</td>    <td>-0.7469</td> <td>0.6054</td> <td>-2.055</td>  <td>0.5611</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_1st</td>  <td>Pre_15th</td>  <td>-1.0195</td> <td>0.0756</td> <td>-2.093</td>  <td>0.0541</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_2nd</td>    <td>20th</td>    <td>0.2046</td>    <td>0.9</td>  <td>-0.4876</td> <td>0.8968</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_2nd</td>    <td>21st</td>    <td>-0.2836</td>   <td>0.9</td>  <td>-1.5071</td> <td>0.9398</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>19th_2nd</td>  <td>Pre_15th</td>  <td>-0.5561</td> <td>0.6002</td> <td>-1.5249</td> <td>0.4126</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>20th</td>      <td>21st</td>    <td>-0.4882</td>   <td>0.9</td>  <td>-1.7185</td>  <td>0.742</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>20th</td>    <td>Pre_15th</td>  <td>-0.7607</td> <td>0.2435</td> <td>-1.7381</td> <td>0.2166</td>  <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>21st</td>    <td>Pre_15th</td>  <td>-0.2725</td>   <td>0.9</td>  <td>-1.6771</td> <td>1.1321</td>  <td>False</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tukey_2 = sts.multicomp.pairwise_tukeyhsd(df_cl.hyp_score, df_cl.date_bins)\n",
    "tukey_2._results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check\n",
    "Since we don't yet have enough data to infer the level of abstraction of each artwork, group artworks based on the average level of hypernyms and sample a few from each group and look at the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
