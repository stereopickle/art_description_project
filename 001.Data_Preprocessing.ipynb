{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "Here I will take the open source object data from Harvard Museum.  \n",
    "(API Documentation and data source: https://www.harvardartmuseums.org/collections/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must request the API key from Harvard Museum by using a link provided in their documentations.  \n",
    "Usually you will receive the key right away.  \n",
    "Then create a harvard_mus_api.json file to store the key as a dictionary.  \n",
    "e.g. {\"api_key\": \"your key here\"}  \n",
    "If you are not uploading this to public and it's for your personal use, you can ignore below step and just assign api_key to your api key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "path = '/Users/stereopickles/.secret' # input the location of your tmdb_api.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = get_keys(f\"{path}/harvard_mus_api.json\")['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.harvardartmuseums.org/object\"\n",
    "\n",
    "url_params = {\n",
    "    \"apikey\": api_key,\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params = url_params)\n",
    "print(resp.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'records'])\n"
     ]
    }
   ],
   "source": [
    "print(resp.json().keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'totalrecordsperquery': 10,\n",
       " 'totalrecords': 234997,\n",
       " 'pages': 23500,\n",
       " 'page': 1,\n",
       " 'next': 'https://api.harvardartmuseums.org/object?apikey=def72120-c45a-11ea-89a3-6722767e4145&page=2'}"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.json()['info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep it to paintings only for the first round. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Paintings'] \n",
    "full_db = []\n",
    "\n",
    "for cls in classes: \n",
    "    url_params = {\n",
    "        \"apikey\": api_key, \n",
    "        \"classification\": cls, \n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, params = url_params)\n",
    "    \n",
    "    if res.status_code == 200: # if connection is successful\n",
    "        # run the rest of the pages\n",
    "        n = int(res.json()['info']['pages']) # getting the page number \n",
    "        \n",
    "        for i in range(2):\n",
    "            url_params[\"page\"] = i\n",
    "\n",
    "            resp = requests.get(url, params = url_params)\n",
    "            \n",
    "            try: \n",
    "                full_db.extend(resp.json()['records']) # add it to the list\n",
    "            except:\n",
    "                print(f\"Error on page {i+1}\") # let me know if there's an error\n",
    "\n",
    "    else: \n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to a Pandas dataframe\n",
    "full_df = pd.DataFrame(full_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop items without tags\n",
    "full_df.dropna(subset = [\"description\"], inplace = True)\n",
    "full_df.description.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Paintings    557\n",
       "Name: classification, dtype: int64"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will preprocess the description data.  \n",
    "We will do  \n",
    "1. Make everything lowercase \n",
    "2. Remove stopwords\n",
    "3. Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = full_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will remove all the stopwords using stopwords corpus from NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK stemming/lemmatizing options\n",
    "I'll quickly run through porter stemmer, lancaster stemmer and worldnetlemmatizer to choose the best option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = ['abstract', 'abstracts', 'abstracted', \n",
    "            'abstracting', 'abstraction', 'women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer: \n",
      "['abstract', 'abstract', 'abstract', 'abstract', 'abstract', 'wom']\n",
      "Porter Stemmer: \n",
      "['abstract', 'abstract', 'abstract', 'abstract', 'abstract', 'women']\n",
      "Worldnet Lemmatizer: \n",
      "['abstract', 'abstract', 'abstracted', 'abstracting', 'abstraction', 'woman']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "print(\"Lancaster Stemmer: \")\n",
    "print([lancaster.stem(x) for x in testlist])\n",
    "print(\"Porter Stemmer: \")\n",
    "print([porter.stem(x) for x in testlist])\n",
    "print(\"Worldnet Lemmatizer: \")\n",
    "print([wnl.lemmatize(x) for x in testlist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like best way might be to run Porter Stemmer first and then running Worldnet Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalizing(string):\n",
    "    \"\"\"\n",
    "    Input: string \n",
    "    Return: list of lower case keywords with special characters removed\n",
    "\n",
    "    \"\"\"\n",
    "    # remove special character, lowercase, then remove individual words\n",
    "    return re.sub('[^A-Za-z]+', ' ', string).lower().split() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# We'll take from NLTK package and add couple more\n",
    "sw = stopwords.words('english')\n",
    "sw += ['p', 'r', 'l', 'x', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(list_):\n",
    "    \"\"\"\n",
    "    Input: list of words\n",
    "    Return: list of words excluding stopwords\n",
    "    \"\"\"\n",
    "    return [x for x in list_ if x not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_keywords(string):\n",
    "    \"\"\"\n",
    "    Input: string of words\n",
    "    Return: list of words excluding stopwords (after normalizing) and lemmatized\n",
    "    \"\"\"\n",
    "    wordslist = remove_stop(normalizing(string))\n",
    "    return list(map(lambda x: wnl.lemmatize(porter.stem(x)), wordslist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.description = clean_df.description.apply(lambda x: make_keywords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking\n",
    "Let's just randomly checks couple samples to ensure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3147    [vasakasajja, nayika, heroin, dress, lover, paint, shown, open, terrac, seat, surround, femal, attend, one, help, shoe, distanc, left, lover, seen, seat, dayb, smoke, hookah, pahari, style, kangra, school]\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "clean_df.sample(1).description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vasakasajja nayika, is a heroine who dresses up for her lover. Here, in this painting, she is shown on an open terrace, seated and surrounded by female attendants, one of whom helps her with her shoes. In the distance, on the left, her lover can be seen seated on a daybed and smoking a hookah. Pahari Style, Kangra School.\n"
     ]
    }
   ],
   "source": [
    "print(full_df.loc[3147, 'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this looks pretty good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the sake of working in separate notebooks, I'll remove the list, and re-split in EDA notebook.  \n",
    "This part can be skipped if it's all in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.description = clean_df.description.apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.to_csv('data/clean_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
