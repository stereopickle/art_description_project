{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "Here I will take the open source object data from Harvard Museum.  \n",
    "(API Documentation and data source: https://www.harvardartmuseums.org/collections/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must request the API key from Harvard Museum by using a link provided in their documentations.  \n",
    "Usually you will receive the key right away.  \n",
    "Then create a harvard_mus_api.json file to store the key as a dictionary.  \n",
    "e.g. {\"api_key\": \"your key here\"}  \n",
    "If you are not uploading this to public and it's for your personal use, you can ignore below step and just assign api_key to your api key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "path = '/Users/stereopickles/.secret' # input the location of your tmdb_api.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = get_keys(f\"{path}/harvard_mus_api.json\")['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.harvardartmuseums.org/object\"\n",
    "\n",
    "url_params = {\n",
    "    \"apikey\": api_key,\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params = url_params)\n",
    "print(resp.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'records'])\n"
     ]
    }
   ],
   "source": [
    "print(resp.json().keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'totalrecordsperquery': 10,\n",
       " 'totalrecords': 234937,\n",
       " 'pages': 23494,\n",
       " 'page': 1,\n",
       " 'next': 'https://api.harvardartmuseums.org/object?apikey=def72120-c45a-11ea-89a3-6722767e4145&page=2'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.json()['info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.harvardartmuseums.org/classification\"\n",
    "\n",
    "url_params = {\n",
    "    \"apikey\": api_key,\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params = url_params)\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "\n",
    "n = int(res.json()['info']['pages']) # getting the page number \n",
    "        \n",
    "for i in range(n):\n",
    "    url_params[\"page\"] = i\n",
    "    print(f\"page {i}\")\n",
    "    \n",
    "    resp = requests.get(url, params = url_params)\n",
    "\n",
    "    try: \n",
    "        classifications.extend(resp.json()['records']) # add it to the list\n",
    "    except:\n",
    "        print(f\"Error on page {i+1}\") # let me know if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectcount</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>lastupdate</th>\n",
       "      <th>classificationid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>Material Specimens</td>\n",
       "      <td>1075</td>\n",
       "      <td>2020-07-27T05:02:40-0400</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Text</td>\n",
       "      <td>204</td>\n",
       "      <td>2020-07-27T05:02:40-0400</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>382</td>\n",
       "      <td>(not assigned)</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-27T05:02:39-0400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>847</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>19</td>\n",
       "      <td>2020-07-27T05:02:39-0400</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6959</td>\n",
       "      <td>Paintings</td>\n",
       "      <td>26</td>\n",
       "      <td>2020-07-27T05:02:40-0400</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>475</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>76</td>\n",
       "      <td>2020-07-27T05:02:39-0400</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1154</td>\n",
       "      <td>Architectural Elements</td>\n",
       "      <td>133</td>\n",
       "      <td>2020-07-27T05:02:39-0400</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>Cameos</td>\n",
       "      <td>1086</td>\n",
       "      <td>2020-07-27T05:02:39-0400</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>16</td>\n",
       "      <td>Graphic Design</td>\n",
       "      <td>171</td>\n",
       "      <td>2020-07-27T05:02:39-0400</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>466</td>\n",
       "      <td>Rubbings</td>\n",
       "      <td>173</td>\n",
       "      <td>2020-07-27T05:02:40-0400</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    objectcount                    name    id                lastupdate  \\\n",
       "0           348      Material Specimens  1075  2020-07-27T05:02:40-0400   \n",
       "1             3                    Text   204  2020-07-27T05:02:40-0400   \n",
       "2           382          (not assigned)     0  2020-07-27T05:02:39-0400   \n",
       "3           847                 Jewelry    19  2020-07-27T05:02:39-0400   \n",
       "4          6959               Paintings    26  2020-07-27T05:02:40-0400   \n",
       "..          ...                     ...   ...                       ...   \n",
       "62          475               Furniture    76  2020-07-27T05:02:39-0400   \n",
       "63         1154  Architectural Elements   133  2020-07-27T05:02:39-0400   \n",
       "64           46                  Cameos  1086  2020-07-27T05:02:39-0400   \n",
       "65           16          Graphic Design   171  2020-07-27T05:02:39-0400   \n",
       "66          466                Rubbings   173  2020-07-27T05:02:40-0400   \n",
       "\n",
       "    classificationid  \n",
       "0               1075  \n",
       "1                204  \n",
       "2                  0  \n",
       "3                 19  \n",
       "4                 26  \n",
       "..               ...  \n",
       "62                76  \n",
       "63               133  \n",
       "64              1086  \n",
       "65               171  \n",
       "66               173  \n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = pd.DataFrame(classifications)\n",
    "classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep it to paintings only for the first round. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Paintings'] \n",
    "full_db = []\n",
    "\n",
    "for cls in classes: \n",
    "    url_params = {\n",
    "        \"apikey\": api_key, \n",
    "        \"classification\": cls, \n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, params = url_params)\n",
    "    \n",
    "    if res.status_code == 200: # if connection is successful\n",
    "        # run the rest of the pages\n",
    "        n = int(res.json()['info']['pages']) # getting the page number \n",
    "        \n",
    "        for i in range(n):\n",
    "            url_params[\"page\"] = i\n",
    "\n",
    "            resp = requests.get(url, params = url_params)\n",
    "            \n",
    "            try: \n",
    "                full_db.extend(resp.json()['records']) # add it to the list\n",
    "            except:\n",
    "                print(f\"Error on page {i+1}\") # let me know if there's an error\n",
    "\n",
    "    else: \n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to a Pandas dataframe\n",
    "full_df = pd.DataFrame(full_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pickles/raw_data.pkl'\n",
    "full_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop items without tags\n",
    "full_df.dropna(subset = [\"description\"], inplace = True)\n",
    "full_df.description.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Paintings    474\n",
       "Name: classification, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will preprocess the description data.  \n",
    "We will do  \n",
    "1. Make everything lowercase \n",
    "2. Remove stopwords\n",
    "3. Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = full_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will remove all the stopwords using stopwords corpus from NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK stemming/lemmatizing options\n",
    "I'll quickly run through porter stemmer, lancaster stemmer and worldnetlemmatizer to choose the best option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = ['abstracts', 'abstracted', 'abstracting', 'abstraction', \n",
    "            'woman', 'women', 'womb', 'made', 'making', 'lying', 'laying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer: ['abstract', 'abstract', 'abstract', 'abstract', 'wom', 'wom', 'womb', 'mad', 'mak', 'lying', 'lay']\n",
      "Porter Stemmer: ['abstract', 'abstract', 'abstract', 'abstract', 'woman', 'women', 'womb', 'made', 'make', 'lie', 'lay']\n",
      "Worldnet Lemmatizer: ['abstract', 'abstracted', 'abstracting', 'abstraction', 'woman', 'woman', 'womb', 'made', 'making', 'lying', 'laying']\n",
      "Worldnet Lemmatizer than Porter Stemmer: ['abstract', 'abstract', 'abstract', 'abstract', 'woman', 'woman', 'womb', 'made', 'make', 'lie', 'lay']\n",
      "Worldnet Lemmatizer than Lancaster Stemmer: ['abstract', 'abstract', 'abstract', 'abstract', 'wom', 'wom', 'womb', 'mad', 'mak', 'lying', 'lay']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "print(f\"Lancaster Stemmer: {[lancaster.stem(x) for x in testlist]}\")\n",
    "print(f\"Porter Stemmer: {[porter.stem(x) for x in testlist]}\")\n",
    "print(f\"Worldnet Lemmatizer: {[wnl.lemmatize(x) for x in testlist]}\")\n",
    "print(f\"Worldnet Lemmatizer than Porter Stemmer: {[porter.stem(wnl.lemmatize(x)) for x in testlist]}\")\n",
    "print(f\"Worldnet Lemmatizer than Lancaster Stemmer: {[lancaster.stem(wnl.lemmatize(x)) for x in testlist]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like best way might be to run Lemmatizer first and run Porter might be our best bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalizing(string):\n",
    "    \"\"\"\n",
    "    Input: string \n",
    "    Return: list of lower case keywords with special characters removed\n",
    "\n",
    "    \"\"\"\n",
    "    # remove special character, lowercase, then remove individual words\n",
    "    return re.sub('[^A-Za-z]+', ' ', string).lower().split() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# We'll take from NLTK package and add couple more\n",
    "sw = stopwords.words('english')\n",
    "sw += ['p', 'r', 'l', 'x', 'e', 'h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(list_):\n",
    "    \"\"\"\n",
    "    Input: list of words\n",
    "    Return: list of words excluding stopwords\n",
    "    \"\"\"\n",
    "    return [x for x in list_ if x not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_keywords(string):\n",
    "    \"\"\"\n",
    "    Input: string of words\n",
    "    Return: list of words excluding stopwords (after normalizing) and lemmatized\n",
    "    \"\"\"\n",
    "    wordslist = remove_stop(normalizing(string))\n",
    "    #return list(map(lambda x: porter.stem(wnl.lemmatize(x)), wordslist))\n",
    "    return list(map(lambda x: wnl.lemmatize(x), wordslist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.description = clean_df.description.apply(lambda x: make_keywords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking\n",
    "Let's just randomly checks couple samples to ensure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1937    [represented, panel, mother, god, bogoluibor, ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(9)\n",
    "clean_df.sample(1).description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Represented on the panel:  the Mother of God of Bogoluibor on the left; three-handed Virgin on the right; St. Stephan with Vlasius and the Archangel Michael on the left; St. Maria of Egypt with Natalia and an unknown saint on the right.  The metal cross has numerous inscriptions which are typical for this type.\n"
     ]
    }
   ],
   "source": [
    "print(full_df.loc[1937, 'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this looks pretty good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the sake of working in separate notebooks, I'll remove the list, and re-split in EDA notebook.  \n",
    "This part can be skipped if it's all in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pickles/cleaned_df.pkl'\n",
    "clean_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.description = clean_df.description.apply(lambda x: ','.join(x))\n",
    "#clean_df.to_csv('data/clean_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
